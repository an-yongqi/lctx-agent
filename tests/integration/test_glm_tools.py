#!/usr/bin/env python3
# Copyright (c) 2025 ByteDance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
GLM Tools Integration Test
Simple integration test for GLM with our tools.
"""

import asyncio
import os
import tempfile
from pathlib import Path

# Add project root to path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from trae_agent.tools.base import ToolCallArguments
from trae_agent.tools.meta_tool import MetaPromptTool
from trae_agent.tools.chunk_tool import TextChunkTool
from trae_agent.utils.config import ModelConfig, ModelProvider
from trae_agent.utils.llm_clients.glm_client import GLMClient


async def test_glm_meta_tool():
    """Test MetaPromptTool with GLM."""
    print("ğŸ”§ Testing MetaPromptTool with GLM-4.5...")
    
    # GLM configuration
    api_key = "e34be58db90244488b2f5ad00008698b.wlAPdq5wKIVuJiNA"
    base_url = "https://open.bigmodel.cn/api/paas/v4/"
    
    model_provider = ModelProvider(
        api_key=api_key,
        provider="glm",
        base_url=base_url
    )
    
    model_config = ModelConfig(
        model="glm-4.5",
        model_provider=model_provider,
        max_tokens=800,
        temperature=0.2,
        top_p=1.0,
        top_k=0,
        parallel_tool_calls=False,
        max_retries=3,
    )
    
    llm_client = GLMClient(model_config)
    tool = MetaPromptTool(model_provider="glm", llm_client=llm_client)
    
    arguments = ToolCallArguments({
        "simple_prompt": "æ€»ç»“æ–‡æ¡£",
        "task_context": "å­¦æœ¯ç ”ç©¶"
    })
    
    try:
        result = await tool.execute(arguments)
        
        if result.error:
            print(f"âŒ MetaTool Error: {result.error}")
            return False
        else:
            print(f"âœ… MetaTool Success!")
            print(f"ğŸ“ Enhanced prompt: {result.output[:100]}...")
            return True
            
    except Exception as e:
        print(f"âŒ MetaTool Exception: {e}")
        return False


async def test_glm_chunk_tool():
    """Test TextChunkTool with GLM."""
    print("\nğŸ”§ Testing TextChunkTool with GLM-4.5...")
    
    # GLM configuration
    api_key = "e34be58db90244488b2f5ad00008698b.wlAPdq5wKIVuJiNA"
    base_url = "https://open.bigmodel.cn/api/paas/v4/"
    
    model_provider = ModelProvider(
        api_key=api_key,
        provider="glm",
        base_url=base_url
    )
    
    model_config = ModelConfig(
        model="glm-4.5",
        model_provider=model_provider,
        max_tokens=800,
        temperature=0.3,
        top_p=1.0,
        top_k=0,
        parallel_tool_calls=False,
        max_retries=3,
    )
    
    llm_client = GLMClient(model_config)
    tool = TextChunkTool(model_provider="glm", llm_client=llm_client)
    
    # Create test file
    test_content = """äººå·¥æ™ºèƒ½å‘å±•å†ç¨‹ç®€è¿°ï¼š
    
1950å¹´ä»£ï¼šè‰¾ä¼¦Â·å›¾çµæå‡ºå›¾çµæµ‹è¯•ã€‚
1956å¹´ï¼šè¾¾ç‰¹èŒ…æ–¯ä¼šè®®ç¡®ç«‹äººå·¥æ™ºèƒ½æ¦‚å¿µã€‚
1960-70å¹´ä»£ï¼šä¸“å®¶ç³»ç»Ÿå…´èµ·ã€‚
1980å¹´ä»£ï¼šæœºå™¨å­¦ä¹ ç†è®ºå‘å±•ã€‚
"""
    
    with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as f:
        f.write(test_content)
        temp_file = f.name
    
    try:
        arguments = ToolCallArguments({
            "file_path": temp_file,
            "start_pos": 0,
            "end_pos": 100,
            "query": "ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "enhanced_prompt": "è¯·ç®€è¦æ¦‚è¿°è¿™æ®µæ–‡å­—çš„ä¸»è¦å†…å®¹ã€‚",
            "chunk_id": "test_chunk"
        })
        
        result = await tool.execute(arguments)
        
        if result.error:
            print(f"âŒ ChunkTool Error: {result.error}")
            return False
        else:
            print(f"âœ… ChunkTool Success!")
            print(f"ğŸ“Š Analysis result: {result.output[:100]}...")
            return True
            
    except Exception as e:
        print(f"âŒ ChunkTool Exception: {e}")
        return False
    finally:
        # Clean up
        if os.path.exists(temp_file):
            os.unlink(temp_file)


async def main():
    """Main test function."""
    print("ğŸš€ GLM Tools Integration Tests")
    print("=" * 50)
    
    results = []
    
    # Test MetaPromptTool
    meta_result = await test_glm_meta_tool()
    results.append(("MetaPromptTool", meta_result))
    
    # Test TextChunkTool
    chunk_result = await test_glm_chunk_tool()
    results.append(("TextChunkTool", chunk_result))
    
    # Summary
    print(f"\n{'=' * 50}")
    print("ğŸ“Š Test Results Summary")
    print("=" * 50)
    
    for tool_name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{tool_name:15}: {status}")
    
    total_passed = sum(r[1] for r in results)
    print(f"\nğŸ¯ Overall: {total_passed}/{len(results)} tests passed")
    
    if total_passed == len(results):
        print("ğŸ‰ All GLM tools integration tests passed!")
    else:
        print("âš ï¸ Some tests failed. Please check the output above.")


if __name__ == "__main__":
    asyncio.run(main())