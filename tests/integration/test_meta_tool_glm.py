# Copyright (c) 2025 ByteDance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

import asyncio
import os
import sys
import unittest
from pathlib import Path
from unittest import SkipTest

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from trae_agent.tools.base import ToolCallArguments
from trae_agent.tools.meta_tool import MetaPromptTool
from trae_agent.utils.config import ModelConfig, ModelProvider
from trae_agent.utils.llm_clients.glm_client import GLMClient


class TestMetaPromptToolWithGLM(unittest.IsolatedAsyncioTestCase):
    """Integration tests for MetaPromptTool using real GLM API."""

    def setUp(self):
        # Skip if running in CI or if we want to avoid API costs
        if os.getenv("SKIP_GLM_TESTS") == "1":
            raise SkipTest("GLM tests skipped (SKIP_GLM_TESTS=1)")
            
        # GLM API configuration
        self.api_key = "e34be58db90244488b2f5ad00008698b.wlAPdq5wKIVuJiNA"
        self.base_url = "https://open.bigmodel.cn/api/paas/v4/"
        
        # Create GLM model config
        model_provider = ModelProvider(
            api_key=self.api_key,
            provider="glm",
            base_url=self.base_url
        )
        
        model_config = ModelConfig(
            model="glm-4.5",
            model_provider=model_provider,
            max_tokens=1500,
            temperature=0.2,  # Lower temperature for more consistent prompt enhancement
            top_p=1.0,
            top_k=0,
            parallel_tool_calls=False,
            max_retries=3,
        )
        
        # Create real GLM client
        llm_client = GLMClient(model_config)
        
        # Create tool with real GLM client
        self.tool = MetaPromptTool(model_provider="glm", llm_client=llm_client)

    async def test_document_summarization_enhancement(self):
        """Test enhancing a document summarization prompt for chunk processing."""
        print(f"\nğŸ”¬ Testing Document Summarization Enhancement with GLM-4.5")
        print("ğŸ¯ Task: Document chunk summarization for sub-agent")
        
        arguments = ToolCallArguments({
            "simple_prompt": "æ€»ç»“è¿™æ®µæ–‡å­—çš„ä¸»è¦å†…å®¹",
            "task_context": "å¤„ç†100é¡µå­¦æœ¯è®ºæ–‡çš„å­æ®µè½ï¼Œéœ€è¦ä¸ºæ¯ä¸ª1000å­—ç‰‡æ®µæå–æ ¸å¿ƒè§‚ç‚¹ï¼Œè¦æ±‚30ç§’å†…å¤„ç†å®Œæˆ"
        })
        
        print(f"ğŸ“ Original prompt: {arguments['simple_prompt']}")
        print(f"ğŸ¯ Context: {arguments['task_context']}")
        print("â³ Enhancing prompt for chunk processing...")
        
        result = await self.tool.execute(arguments)
        
        # Verify the enhanced prompt
        self.assertIsNone(result.error, f"Tool execution failed: {result.error}")
        self.assertIsNotNone(result.output)
        self.assertGreater(len(result.output), len(arguments["simple_prompt"]))
        
        print(f"âœ… Enhanced Prompt Result:")
        print(f"ğŸ“Š Original length: {len(arguments['simple_prompt'])} characters")
        print(f"ğŸ“ˆ Enhanced length: {len(result.output)} characters")
        print(f"ğŸ¤– GLM Enhanced Prompt:")
        print("-" * 50)
        print(result.output)
        print("-" * 50)
        
        # Check for chunk-specific and academic processing instructions
        academic_terms = ["ç‰‡æ®µ", "chunk", "ç®€æ´", "å…³é”®", "è§‚ç‚¹", "æ ¸å¿ƒ", "å­¦æœ¯", "è®ºæ–‡", "æ®µè½"]
        found_terms = [term for term in academic_terms if term in result.output]
        self.assertGreater(len(found_terms), 2,
                          f"Enhanced prompt should contain academic chunk processing instructions, found: {found_terms}")

    async def test_information_extraction_enhancement(self):
        """Test enhancing an information extraction prompt for chunk processing."""
        print(f"\nğŸ”¬ Testing Information Extraction Enhancement with GLM-4.5")
        
        arguments = ToolCallArguments({
            "simple_prompt": "æå–é‡è¦ä¿¡æ¯",
            "task_context": "ä»500é¡µæŠ€æœ¯æ‰‹å†Œä¸­æå–APIæ¥å£ä¿¡æ¯ï¼Œæ¯ä¸ªç‰‡æ®µ500å­—ï¼Œéœ€è¦è¯†åˆ«å‡½æ•°åã€å‚æ•°ã€è¿”å›å€¼"
        })
        
        print(f"ğŸ” Extraction prompt: {arguments['simple_prompt']}")
        print("â³ Enhancing extraction prompt for sub-agent...")
        
        result = await self.tool.execute(arguments)
        
        # Verify extraction enhancement
        self.assertIsNone(result.error, f"Tool execution failed: {result.error}")
        self.assertIsNotNone(result.output)
        self.assertGreater(len(result.output), 50)
        
        # Check for API extraction and technical terms
        api_terms = ["æå–", "API", "æ¥å£", "å‡½æ•°", "å‚æ•°", "è¿”å›å€¼", "æŠ€æœ¯", "æ‰‹å†Œ", "ç‰‡æ®µ"]
        found_terms = [term for term in api_terms if term in result.output]
        
        self.assertGreater(len(found_terms), 3,
                          f"API extraction prompt should contain relevant terms: {found_terms}")
        
        print(f"âœ… Technical API Extraction Enhancement Result:")
        print(f"ğŸ” Found API extraction terms: {found_terms}")
        print(f"ğŸ¤– Enhanced API Extraction Prompt:")
        print("-" * 50)
        print(result.output)
        print("-" * 50)

    async def test_classification_task_enhancement(self):
        """Test enhancing a classification prompt for chunk processing."""
        print(f"\nğŸ”¬ Testing Classification Task Enhancement with GLM-4.5")
        
        arguments = ToolCallArguments({
            "simple_prompt": "åˆ¤æ–­æ–‡æœ¬ç±»å‹",
            "task_context": "å¯¹å®¢æœèŠå¤©è®°å½•è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼Œæ¯æ®µ300å­—å¯¹è¯ç‰‡æ®µï¼Œéœ€åˆ†ç±»ä¸ºç§¯æ/æ¶ˆæ/ä¸­æ€§ï¼Œå¹¶è¡Œå¤„ç†1000ä¸ªç‰‡æ®µ"
        })
        
        print(f"ğŸ·ï¸ Classification prompt: {arguments['simple_prompt']}")
        print("â³ Enhancing classification prompt for chunk processing...")
        
        result = await self.tool.execute(arguments)
        
        # Verify classification enhancement
        self.assertIsNone(result.error, f"Tool execution failed: {result.error}")
        self.assertIsNotNone(result.output)
        self.assertGreater(len(result.output), 50)
        
        # Check for sentiment classification and customer service terms
        sentiment_terms = ["åˆ†ç±»", "æƒ…æ„Ÿ", "ç§¯æ", "æ¶ˆæ", "ä¸­æ€§", "å®¢æœ", "å¯¹è¯", "å¹¶è¡Œ", "ç‰‡æ®µ"]
        found_terms = [term for term in sentiment_terms if term in result.output]
        
        self.assertGreater(len(found_terms), 3,
                          f"Sentiment classification prompt should contain relevant terms: {found_terms}")
        
        print(f"âœ… Customer Service Sentiment Classification Enhancement Result:")
        print(f"ğŸ·ï¸ Found sentiment classification terms: {found_terms}")
        print(f"ğŸ¤– Enhanced Customer Service Classification Prompt:")
        print("-" * 50)
        print(result.output)
        print("-" * 50)

    async def test_entity_recognition_enhancement(self):
        """Test enhancing an entity recognition prompt for chunk processing."""
        print(f"\nğŸ”¬ Testing Entity Recognition Enhancement with GLM-4.5")
        
        arguments = ToolCallArguments({
            "simple_prompt": "è¯†åˆ«é‡è¦å®ä½“",
            "task_context": "ä»æ–°é—»è¯­æ–™åº“æå–å‘½åå®ä½“ï¼Œæ¯ä¸ª800å­—æ–°é—»ç‰‡æ®µè¯†åˆ«äººåã€å…¬å¸åã€åœ°åï¼Œéœ€è¦å¤„ç†å¯èƒ½æˆªæ–­çš„å®ä½“è¾¹ç•Œ"
        })
        
        print(f"ğŸƒ Entity recognition prompt: {arguments['simple_prompt']}")
        print("â³ Enhancing entity recognition prompt for chunk processing...")
        
        result = await self.tool.execute(arguments)
        
        # Verify entity recognition enhancement
        self.assertIsNone(result.error, f"Tool execution failed: {result.error}")
        self.assertIsNotNone(result.output)
        self.assertGreater(len(result.output), 50)
        
        # Check for news entity recognition and boundary handling terms
        news_entity_terms = ["å®ä½“", "æ–°é—»", "äººå", "å…¬å¸", "åœ°å", "è¾¹ç•Œ", "æˆªæ–­", "è¯­æ–™", "ç‰‡æ®µ"]
        found_terms = [term for term in news_entity_terms if term in result.output]
        
        self.assertGreater(len(found_terms), 3,
                          f"News entity recognition prompt should contain relevant terms: {found_terms}")
        
        print(f"âœ… News Entity Recognition with Boundary Handling Result:")
        print(f"ğŸƒ Found news entity terms: {found_terms}")
        print(f"ğŸ¤– Enhanced News Entity Recognition Prompt:")
        print("-" * 50)
        print(result.output)
        print("-" * 50)

    async def test_sentiment_analysis_enhancement(self):
        """Test enhancing a sentiment analysis prompt for chunk processing."""
        print(f"\nğŸ”¬ Testing Sentiment Analysis Enhancement with GLM-4.5")
        
        arguments = ToolCallArguments({
            "simple_prompt": "åˆ†ææƒ…æ„Ÿå€¾å‘",
            "task_context": "å¯¹ç”µå•†è¯„è®ºè¿›è¡Œæƒ…æ„ŸæŒ–æ˜ï¼Œæ¯ä¸ª200å­—è¯„è®ºç‰‡æ®µï¼Œè¾“å‡ºæƒ…æ„Ÿå¾—åˆ†å’Œå…³é”®è¯ï¼Œ10ç§’å†…å®Œæˆå•ç‰‡æ®µåˆ†æ"
        })
        
        print(f"ğŸ˜Š Sentiment analysis prompt: {arguments['simple_prompt']}")
        print(f"ğŸ“„ Context: {arguments['task_context']}")
        print("â³ Enhancing sentiment analysis prompt for chunk processing...")
        
        result = await self.tool.execute(arguments)
        
        # Verify sentiment analysis enhancement
        self.assertIsNone(result.error, f"Tool execution failed: {result.error}")
        self.assertIsNotNone(result.output)
        self.assertGreater(len(result.output), 50)
        
        # Check for e-commerce sentiment analysis terms
        ecommerce_terms = ["æƒ…æ„Ÿ", "ç”µå•†", "è¯„è®º", "å¾—åˆ†", "å…³é”®è¯", "æŒ–æ˜", "ç‰‡æ®µ", "åˆ†æ"]
        found_terms = [term for term in ecommerce_terms if term in result.output]
        
        self.assertGreater(len(found_terms), 3,
                          f"E-commerce sentiment analysis prompt should contain relevant terms: {found_terms}")
        
        print(f"âœ… E-commerce Review Sentiment Mining Enhancement Result:")
        print(f"ğŸ˜Š Found e-commerce sentiment terms: {found_terms}")
        print(f"ğŸ¤– Enhanced E-commerce Sentiment Mining Prompt:")
        print("-" * 50)
        print(result.output)
        print("-" * 50)

    async def test_chunk_boundary_handling_enhancement(self):
        """Test enhancing a prompt for handling chunk boundary cases."""
        print(f"\nğŸ”¬ Testing Chunk Boundary Handling Enhancement with GLM-4.5")
        
        # Test chunk boundary handling prompt
        arguments = ToolCallArguments({
            "simple_prompt": "å¤„ç†ä¸å®Œæ•´çš„æ–‡æœ¬",
            "task_context": "å¤„ç†æ³•å¾‹åˆåŒæ¡æ¬¾åˆ‡åˆ†ï¼Œæ¯ä¸ª1200å­—ç‰‡æ®µå¯èƒ½åœ¨æ¡æ¬¾ä¸­é—´æˆªæ–­ï¼Œéœ€è¦æ ‡è®°ä¸å®Œæ•´ä¿¡æ¯å¹¶æŒ‡ç¤ºåç»­å¤„ç†"
        })
        
        print("ğŸ”€ Testing chunk boundary handling enhancement...")
        print(f"ğŸ“ Original prompt: {arguments['simple_prompt']}")
        print(f"ğŸ¯ Context: {arguments['task_context']}")
        
        result1 = await self.tool.execute(arguments)
        result2 = await self.tool.execute(arguments)
        
        # Both results should be meaningful
        self.assertIsNone(result1.error, f"First result failed: {result1.error}")
        self.assertIsNone(result2.error, f"Second result failed: {result2.error}")
        self.assertIsNotNone(result1.output)
        self.assertIsNotNone(result2.output)
        self.assertGreater(len(result1.output), 50)
        self.assertGreater(len(result2.output), 50)
        
        # Check for legal contract boundary handling terms
        legal_terms = ["æ³•å¾‹", "åˆåŒ", "æ¡æ¬¾", "æˆªæ–­", "è¾¹ç•Œ", "æ ‡è®°", "ä¸å®Œæ•´", "åç»­", "ç‰‡æ®µ"]
        found_in_result1 = [term for term in legal_terms if term in result1.output]
        found_in_result2 = [term for term in legal_terms if term in result2.output]
        
        self.assertGreater(len(found_in_result1), 2)
        self.assertGreater(len(found_in_result2), 2)
        
        print(f"âœ… Legal Contract Chunk Boundary Enhancement Results:")
        print(f"ğŸ“ First result length: {len(result1.output)}")
        print(f"ğŸ“ Second result length: {len(result2.output)}")
        print(f"ğŸ”€ Legal boundary terms in result 1: {found_in_result1}")
        print(f"ğŸ”€ Legal boundary terms in result 2: {found_in_result2}")
        print(f"ğŸ¤– Enhanced Legal Contract Boundary Handling Prompt:")
        print("-" * 50)
        print(result1.output)
        print("-" * 50)


if __name__ == "__main__":
    # Note: These tests make real API calls to GLM
    # Set SKIP_GLM_TESTS=1 to skip them
    unittest.main()