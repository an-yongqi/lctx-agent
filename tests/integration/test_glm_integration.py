#!/usr/bin/env python3
# Copyright (c) 2025 ByteDance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
GLM (ChatGLM) integration test script.
This script tests the GLM client with real API calls.
"""

import asyncio
import sys
from pathlib import Path

# Add the project root to Python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from trae_agent.utils.config import ModelConfig, ModelProvider
from trae_agent.utils.llm_clients.llm_client import LLMClient
from trae_agent.utils.llm_clients.llm_basics import LLMMessage


async def test_glm_basic_chat():
    """Test basic chat functionality with GLM."""
    
    print("ğŸ§ª Testing GLM Basic Chat")
    print("=" * 50)
    
    # Configure GLM client
    model_provider = ModelProvider(
        api_key="e34be58db90244488b2f5ad00008698b.wlAPdq5wKIVuJiNA",
        provider="glm",
        base_url="https://open.bigmodel.cn/api/paas/v4/"
    )
    
    model_config = ModelConfig(
        model="glm-4.5",  # Use fastest model for testing
        model_provider=model_provider,
        max_tokens=500,
        temperature=0.7,
        top_p=1.0,
        top_k=0,
        parallel_tool_calls=False,
        max_retries=3,
    )
    
    # Create client
    client = LLMClient(model_config)
    
    # Test simple conversation
    messages = [
        LLMMessage(role="user", content="ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ChatGLMæ¨¡å‹ã€‚")
    ]
    
    print("ğŸ’¬ å‘é€æ¶ˆæ¯: ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ChatGLMæ¨¡å‹ã€‚")
    print("â³ ç­‰å¾…å“åº”...")
    
    try:
        response = client.chat(messages, model_config)
        
        print("\nâœ… å“åº”æˆåŠŸ!")
        print(f"ğŸ¤– GLMå›å¤: {response.content}")
        
        if response.usage:
            print(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:")
            print(f"   è¾“å…¥tokens: {response.usage.input_tokens}")
            print(f"   è¾“å‡ºtokens: {response.usage.output_tokens}")
        
        print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {response.model}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {str(e)}")
        return False


async def test_glm_with_tools():
    """Test GLM with tool calling (if supported)."""
    
    print("\nğŸ”§ Testing GLM Tool Calling")
    print("=" * 50)
    
    model_provider = ModelProvider(
        api_key="e34be58db90244488b2f5ad00008698b.wlAPdq5wKIVuJiNA",
        provider="glm",
        base_url="https://open.bigmodel.cn/api/paas/v4/"
    )
    
    model_config = ModelConfig(
        model="glm-4.5",  # Use more advanced model for tool calling
        model_provider=model_provider,
        max_tokens=1000,
        temperature=0.3,
        top_p=1.0,
        top_k=0,
        parallel_tool_calls=False,
        max_retries=3,
    )
    
    client = LLMClient(model_config)
    
    # Simple tool-like conversation
    messages = [
        LLMMessage(role="user", content="è¯·å¸®æˆ‘è®¡ç®— 15 * 23 + 45 çš„ç»“æœï¼Œå¹¶è§£é‡Šè®¡ç®—æ­¥éª¤ã€‚")
    ]
    
    print("ğŸ’¬ å‘é€æ¶ˆæ¯: è¯·å¸®æˆ‘è®¡ç®— 15 * 23 + 45 çš„ç»“æœï¼Œå¹¶è§£é‡Šè®¡ç®—æ­¥éª¤ã€‚")
    print("â³ ç­‰å¾…å“åº”...")
    
    try:
        response = client.chat(messages, model_config)
        
        print("\nâœ… å“åº”æˆåŠŸ!")
        print(f"ğŸ¤– GLMå›å¤: {response.content}")
        
        if response.usage:
            print(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:")
            print(f"   è¾“å…¥tokens: {response.usage.input_tokens}")
            print(f"   è¾“å‡ºtokens: {response.usage.output_tokens}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {str(e)}")
        return False


async def test_long_context_agent_with_glm():
    """Test our LongContextAgent with GLM."""
    
    print("\nğŸš€ Testing LongContextAgent with GLM")
    print("=" * 50)
    
    from trae_agent.agent.long_context_agent import LongContextAgent
    from trae_agent.utils.config import AgentConfig
    
    # Configure GLM for long context agent
    model_provider = ModelProvider(
        api_key="e34be58db90244488b2f5ad00008698b.wlAPdq5wKIVuJiNA",
        provider="glm",
        base_url="https://open.bigmodel.cn/api/paas/v4/"
    )
    
    model_config = ModelConfig(
        model="glm-4.5",
        model_provider=model_provider,
        max_tokens=2000,
        temperature=0.3,
        top_p=1.0,
        top_k=0,
        parallel_tool_calls=False,
        max_retries=3,
    )
    
    agent_config = AgentConfig(
        model=model_config,
        max_steps=50,
        tools=[],  # Will be set by LongContextAgent
        allow_mcp_servers=[],
        mcp_servers_config={},
    )
    
    # Create agent
    agent = LongContextAgent(agent_config)
    
    # Simple long context test
    long_context = """
    äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥åˆ†ä¸ºå‡ ä¸ªé‡è¦é˜¶æ®µï¼š

    1950å¹´ä»£ï¼šè‰¾ä¼¦Â·å›¾çµæå‡ºå›¾çµæµ‹è¯•ï¼Œæ ‡å¿—ç€äººå·¥æ™ºèƒ½ç†è®ºåŸºç¡€çš„å»ºç«‹ã€‚
    1956å¹´ï¼šè¾¾ç‰¹èŒ…æ–¯ä¼šè®®æ­£å¼æå‡º"äººå·¥æ™ºèƒ½"è¿™ä¸€æ¦‚å¿µã€‚
    1960-70å¹´ä»£ï¼šä¸“å®¶ç³»ç»Ÿå…´èµ·ï¼Œå¦‚MYCINåŒ»ç–—è¯Šæ–­ç³»ç»Ÿã€‚
    1980å¹´ä»£ï¼šæœºå™¨å­¦ä¹ ç†è®ºé€æ¸å‘å±•ï¼Œç¥ç»ç½‘ç»œé‡è·å…³æ³¨ã€‚
    1990å¹´ä»£ï¼šäº’è”ç½‘æ™®åŠæ¨åŠ¨äº†æ•°æ®ç§¯ç´¯å’Œç®—æ³•æ”¹è¿›ã€‚
    2000å¹´ä»£ï¼šæ·±åº¦å­¦ä¹ çªç ´ï¼Œå›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†å–å¾—é‡å¤§è¿›å±•ã€‚
    2010å¹´ä»£ï¼šå¤§è¯­è¨€æ¨¡å‹å‡ºç°ï¼Œå¦‚GPTã€BERTç­‰ã€‚
    2020å¹´ä»£ï¼šChatGPTç­‰å¯¹è¯å¼AIå¹¿æ³›åº”ç”¨ï¼ŒAGIæˆä¸ºæ–°ç›®æ ‡ã€‚

    äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å„è¡Œä¸šçš„åº”ç”¨åŒ…æ‹¬ï¼š
    - åŒ»ç–—ï¼šè¾…åŠ©è¯Šæ–­ã€è¯ç‰©å‘ç°ã€ä¸ªæ€§åŒ–æ²»ç–—
    - é‡‘èï¼šé£é™©è¯„ä¼°ã€ç®—æ³•äº¤æ˜“ã€å®¢æˆ·æœåŠ¡  
    - æ•™è‚²ï¼šä¸ªæ€§åŒ–å­¦ä¹ ã€æ™ºèƒ½è¾…å¯¼
    - äº¤é€šï¼šè‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å¯¼èˆª
    - åˆ¶é€ ä¸šï¼šè´¨é‡æ§åˆ¶ã€é¢„æµ‹æ€§ç»´æŠ¤
    - å¨±ä¹ï¼šå†…å®¹æ¨èã€æ¸¸æˆAI

    æœªæ¥å‘å±•è¶‹åŠ¿åŒ…æ‹¬æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ã€å¤šæ¨¡æ€èåˆã€è¾¹ç¼˜è®¡ç®—éƒ¨ç½²ç­‰ã€‚
    """ * 3  # é‡å¤å‡ æ¬¡ä½¿å†…å®¹æ›´é•¿
    
    query = "è¯·æ€»ç»“äººå·¥æ™ºèƒ½å‘å±•çš„ä¸»è¦é˜¶æ®µï¼Œå¹¶åˆ†æå…¶åœ¨ä¸åŒè¡Œä¸šçš„åº”ç”¨æƒ…å†µã€‚"
    
    print(f"ğŸ“„ é•¿æ–‡æœ¬é•¿åº¦: {len(long_context):,} å­—ç¬¦")
    print(f"â“ æŸ¥è¯¢: {query}")
    print("â³ LongContextAgent å¤„ç†ä¸­...")
    
    try:
        task = "åˆ†æäººå·¥æ™ºèƒ½å‘å±•å†ç¨‹å¹¶æ€»ç»“åº”ç”¨é¢†åŸŸ"
        extra_args = {
            "context": long_context,
            "query": query
        }
        
        agent.new_task(task, extra_args)
        execution = await agent.execute_task()
        
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"ğŸ¯ æˆåŠŸçŠ¶æ€: {execution.success}")
        print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {execution.execution_time:.2f} ç§’")
        print(f"ğŸ”¢ æ‰§è¡Œæ­¥æ•°: {len(execution.steps)}")
        print(f"ğŸ“ æœ€ç»ˆç»“æœ:\n{execution.final_result}")
        
        return execution.success
        
    except Exception as e:
        print(f"âŒ LongContextAgent æ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Main test function."""
    
    print("ğŸ”¬ GLM (ChatGLM) Integration Tests")
    print("=" * 60)
    print("ğŸ”‘ ä½¿ç”¨API Key: e34be58db90244488b2f5ad00008698b.wlAPdq5wKIVuJiNA")
    print("ğŸŒ Base URL: https://open.bigmodel.cn/api/paas/v4/")
    print()
    
    results = []
    
    # Test 1: Basic chat
    result1 = await test_glm_basic_chat()
    results.append(("Basic Chat", result1))
    
    # Test 2: Advanced conversation
    result2 = await test_glm_with_tools()
    results.append(("Advanced Conversation", result2))
    
    # Test 3: Long Context Agent
    result3 = await test_long_context_agent_with_glm()
    results.append(("LongContextAgent", result3))
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    for test_name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{test_name:20}: {status}")
    
    total_passed = sum(results[i][1] for i in range(len(results)))
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {total_passed}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if total_passed == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼GLM é›†æˆæˆåŠŸï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    asyncio.run(main())